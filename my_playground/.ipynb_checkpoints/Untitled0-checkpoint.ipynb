{
 "metadata": {
  "name": "",
  "signature": "sha256:1d62c3a0f4a1a5fd15bf574fb8f2524a7a4167fde516a4a2be1a7806c584e865"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn\n",
      "from sklearn import ensemble, datasets, cross_validation, metrics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "boston = datasets.load_boston()\n",
      "boston.data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "(506, 13)"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "friedman1 = datasets.make_friedman1(n_samples=10000, n_features=20, noise=1.)\n",
      "friedman1\n",
      "friedman1[0].shape\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "(10000, 20)"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = friedman1[0]\n",
      "y = friedman1[1]\n",
      "X_train, X_valid, y_train, y_valid = cross_validation.train_test_split(X, y, train_size=0.7)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "(7000, 20)"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def regression_report(y_true, y_pred):\n",
      "    print '\\t explained variance:', metrics.regression.explained_variance_score(y_true, y_pred)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model_gbc = ensemble.GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.1, loss='ls',\n",
      "             max_depth=8, \n",
      "             max_features=None, \n",
      "             #max_leaf_nodes=15,\n",
      "             min_samples_leaf=20, min_samples_split=40,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=70,\n",
      "             random_state=None, subsample=0.8, tree_params_producer=None,\n",
      "             verbose=1, warm_start=False)\n",
      "\n",
      "model_gbc.fit(X_train, y_train)\n",
      "\n",
      "\n",
      "print ' '\n",
      "print 'Train:'\n",
      "regression_report(y_train, model_gbc.predict(X_train))\n",
      "print ' '\n",
      "print 'Test:'\n",
      "regression_report(y_valid, model_gbc.predict(X_valid))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "      Iter       Train Loss      OOB Improve   Remaining Time \n",
        "         1          20.9019           3.7085            6.56s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         2          17.7793           3.0312            6.10s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         3          15.0250           2.4743            5.98s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         4          12.7851           2.0418            5.79s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         5          10.7326           1.8724            5.64s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         6           9.2756           1.4604            5.51s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         7           8.0677           1.1391            5.41s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         8           6.8960           1.0195            5.29s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         9           6.0294           0.8592            5.18s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "        10           5.3834           0.6615            5.06s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "        20           1.8812           0.1390            4.02s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "        30           0.9802           0.0279            3.14s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "        40           0.7031           0.0037            2.32s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "        50           0.5863           0.0015            1.52s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "        60           0.5097          -0.0003            0.76s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "        70           0.4748          -0.0010            0.00s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " \n",
        "Train:\n",
        "\t explained variance: 0.980751373945\n",
        " \n",
        "Test:\n",
        "\t explained variance: 0.940881259745\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def linear_variable(vfrom, vto, N):\n",
      "    return lambda i: int(i * 1. / N * (vfrom - vto) + vfrom)\n",
      "\n",
      "\n",
      "def tree_params_producer_variable_depth(depth_from, depth_to, n_estimators):\n",
      "    variable_depth_foo = linear_variable(depth_from, depth_to, n_estimators)\n",
      "    return lambda stage: {\n",
      "            'max_depth': variable_depth_foo(stage),\n",
      "            'min_samples_split': 2,\n",
      "            'min_samples_leaf': 1,\n",
      "            'min_weight_fraction_leaf': 0.0,                \n",
      "            'max_features': None,\n",
      "            'max_leaf_nodes': None}\n",
      "\n",
      "\n",
      "model_gbc = ensemble.GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.1, loss='ls',\n",
      "             #max_depth=8, \n",
      "             #max_features=None, \n",
      "             #max_leaf_nodes=15,\n",
      "             #min_samples_leaf=20, min_samples_split=40,\n",
      "             #min_weight_fraction_leaf=0.0, \n",
      "             n_estimators=70,\n",
      "             random_state=None, \n",
      "             subsample=0.8, \n",
      "             tree_params_producer=tree_params_producer_variable_depth(1, 8, 70),\n",
      "             verbose=1, warm_start=False)\n",
      "\n",
      "model_gbc.fit(X_train, y_train)\n",
      "\n",
      "\n",
      "print ' '\n",
      "print 'Train:'\n",
      "regression_report(y_train, model_gbc.predict(X_train))\n",
      "print ' '\n",
      "print 'Test:'\n",
      "regression_report(y_valid, model_gbc.predict(X_valid))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "      Iter       Train Loss      OOB Improve   Remaining Time \n",
        "         1          20.6301           3.9689            8.02s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "max_depth must be greater than zero. ",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-13-b2c5db4be71d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m              verbose=1, warm_start=False)\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mmodel_gbc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/obus/.local/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    989\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    990\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[1;32m--> 991\u001b[1;33m                                     begin_at_stage, monitor)\n\u001b[0m\u001b[0;32m    992\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/obus/.local/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m   1049\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[0;32m   1050\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m                                      random_state)\n\u001b[0m\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/obus/.local/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, y_pred, sample_weight, sample_mask, criterion, splitter, random_state)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[1;32m--> 777\u001b[1;33m                      check_input=False)\n\u001b[0m\u001b[0;32m    778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/obus/.local/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"min_weight_fraction_leaf must in [0, 0.5]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max_depth must be greater than zero. \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max_features must be in (0, n_features]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: max_depth must be greater than zero. "
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(metrics.regression)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on module sklearn.metrics.regression in sklearn.metrics:\n",
        "\n",
        "NAME\n",
        "    sklearn.metrics.regression - Metrics to assess performance on regression task\n",
        "\n",
        "FILE\n",
        "    /home/obus/.local/lib/python2.7/site-packages/sklearn/metrics/regression.py\n",
        "\n",
        "DESCRIPTION\n",
        "    Functions named as ``*_score`` return a scalar value to maximize: the higher\n",
        "    the better\n",
        "    \n",
        "    Function named as ``*_error`` or ``*_loss`` return a scalar value to minimize:\n",
        "    the lower the better\n",
        "\n",
        "FUNCTIONS\n",
        "    explained_variance_score(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
        "        Explained variance regression score function\n",
        "        \n",
        "        Best possible score is 1.0, lower values are worse.\n",
        "        \n",
        "        Read more in the :ref:`User Guide <explained_variance_score>`.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
        "            Ground truth (correct) target values.\n",
        "        \n",
        "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
        "            Estimated target values.\n",
        "        \n",
        "        sample_weight : array-like of shape = (n_samples), optional\n",
        "            Sample weights.\n",
        "        \n",
        "        multioutput : string in ['raw_values', 'uniform_average',                 'variance_weighted'] or array-like of shape (n_outputs)\n",
        "            Defines aggregating of multiple output scores.\n",
        "            Array-like value defines weights used to average scores.\n",
        "        \n",
        "            'raw_values' :\n",
        "                Returns a full set of scores in case of multioutput input.\n",
        "        \n",
        "            'uniform_average' :\n",
        "                Scores of all outputs are averaged with uniform weight.\n",
        "        \n",
        "            'variance_weighted' :\n",
        "                Scores of all outputs are averaged, weighted by the variances\n",
        "                of each individual output.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        score : float or ndarray of floats\n",
        "            The explained variance or ndarray if 'multioutput' is 'raw_values'.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This is not a symmetric function.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> from sklearn.metrics import explained_variance_score\n",
        "        >>> y_true = [3, -0.5, 2, 7]\n",
        "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
        "        >>> explained_variance_score(y_true, y_pred)  # doctest: +ELLIPSIS\n",
        "        0.957...\n",
        "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
        "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
        "        >>> explained_variance_score(y_true, y_pred, multioutput='uniform_average')\n",
        "        ... # doctest: +ELLIPSIS\n",
        "        0.983...\n",
        "    \n",
        "    mean_absolute_error(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
        "        Mean absolute error regression loss\n",
        "        \n",
        "        Read more in the :ref:`User Guide <mean_absolute_error>`.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
        "            Ground truth (correct) target values.\n",
        "        \n",
        "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
        "            Estimated target values.\n",
        "        \n",
        "        sample_weight : array-like of shape = (n_samples), optional\n",
        "            Sample weights.\n",
        "        \n",
        "        multioutput : string in ['raw_values', 'uniform_average']\n",
        "            or array-like of shape (n_outputs)\n",
        "            Defines aggregating of multiple output values.\n",
        "            Array-like value defines weights used to average errors.\n",
        "        \n",
        "            'raw_values' :\n",
        "                Returns a full set of errors in case of multioutput input.\n",
        "        \n",
        "            'uniform_average' :\n",
        "                Errors of all outputs are averaged with uniform weight.\n",
        "        \n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        loss : float or ndarray of floats\n",
        "            If multioutput is 'raw_values', then mean absolute error is returned\n",
        "            for each output separately.\n",
        "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
        "            weighted average of all output errors is returned.\n",
        "        \n",
        "            MAE output is non-negative floating point. The best value is 0.0.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> from sklearn.metrics import mean_absolute_error\n",
        "        >>> y_true = [3, -0.5, 2, 7]\n",
        "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
        "        >>> mean_absolute_error(y_true, y_pred)\n",
        "        0.5\n",
        "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
        "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
        "        >>> mean_absolute_error(y_true, y_pred)\n",
        "        0.75\n",
        "        >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
        "        array([ 0.5,  1. ])\n",
        "        >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
        "        ... # doctest: +ELLIPSIS\n",
        "        0.849...\n",
        "    \n",
        "    mean_squared_error(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
        "        Mean squared error regression loss\n",
        "        \n",
        "        Read more in the :ref:`User Guide <mean_squared_error>`.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
        "            Ground truth (correct) target values.\n",
        "        \n",
        "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
        "            Estimated target values.\n",
        "        \n",
        "        sample_weight : array-like of shape = (n_samples), optional\n",
        "            Sample weights.\n",
        "        \n",
        "        multioutput : string in ['raw_values', 'uniform_average']\n",
        "            or array-like of shape (n_outputs)\n",
        "            Defines aggregating of multiple output values.\n",
        "            Array-like value defines weights used to average errors.\n",
        "        \n",
        "            'raw_values' :\n",
        "                Returns a full set of errors in case of multioutput input.\n",
        "        \n",
        "            'uniform_average' :\n",
        "                Errors of all outputs are averaged with uniform weight.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        loss : float or ndarray of floats\n",
        "            A non-negative floating point value (the best value is 0.0), or an\n",
        "            array of floating point values, one for each individual target.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> from sklearn.metrics import mean_squared_error\n",
        "        >>> y_true = [3, -0.5, 2, 7]\n",
        "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
        "        >>> mean_squared_error(y_true, y_pred)\n",
        "        0.375\n",
        "        >>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
        "        >>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
        "        >>> mean_squared_error(y_true, y_pred)  # doctest: +ELLIPSIS\n",
        "        0.708...\n",
        "        >>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
        "        ... # doctest: +ELLIPSIS\n",
        "        array([ 0.416...,  1.        ])\n",
        "        >>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
        "        ... # doctest: +ELLIPSIS\n",
        "        0.824...\n",
        "    \n",
        "    median_absolute_error(y_true, y_pred)\n",
        "        Median absolute error regression loss\n",
        "        \n",
        "        Read more in the :ref:`User Guide <median_absolute_error>`.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        y_true : array-like of shape = (n_samples)\n",
        "            Ground truth (correct) target values.\n",
        "        \n",
        "        y_pred : array-like of shape = (n_samples)\n",
        "            Estimated target values.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        loss : float\n",
        "            A positive floating point value (the best value is 0.0).\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> from sklearn.metrics import median_absolute_error\n",
        "        >>> y_true = [3, -0.5, 2, 7]\n",
        "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
        "        >>> median_absolute_error(y_true, y_pred)\n",
        "        0.5\n",
        "    \n",
        "    r2_score(y_true, y_pred, sample_weight=None, multioutput=None)\n",
        "        R^2 (coefficient of determination) regression score function.\n",
        "        \n",
        "        Best possible score is 1.0 and it can be negative (because the\n",
        "        model can be arbitrarily worse). A constant model that always\n",
        "        predicts the expected value of y, disregarding the input features,\n",
        "        would get a R^2 score of 0.0.\n",
        "        \n",
        "        Read more in the :ref:`User Guide <r2_score>`.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
        "            Ground truth (correct) target values.\n",
        "        \n",
        "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
        "            Estimated target values.\n",
        "        \n",
        "        sample_weight : array-like of shape = (n_samples), optional\n",
        "            Sample weights.\n",
        "        \n",
        "        multioutput : string in ['raw_values', 'uniform_average',\n",
        "                    'variance_weighted'] or None or array-like of shape (n_outputs)\n",
        "            Defines aggregating of multiple output scores.\n",
        "            Array-like value defines weights used to average scores.\n",
        "            Default value correponds to 'variance_weighted', but\n",
        "            will be changed to 'uniform_average' in next versions.\n",
        "        \n",
        "            'raw_values' :\n",
        "                Returns a full set of scores in case of multioutput input.\n",
        "        \n",
        "            'uniform_average' :\n",
        "                Scores of all outputs are averaged with uniform weight.\n",
        "        \n",
        "            'variance_weighted' :\n",
        "                Scores of all outputs are averaged, weighted by the variances\n",
        "                of each individual output.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        z : float or ndarray of floats\n",
        "            The R^2 score or ndarray of scores if 'multioutput' is\n",
        "            'raw_values'.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This is not a symmetric function.\n",
        "        \n",
        "        Unlike most other scores, R^2 score may be negative (it need not actually\n",
        "        be the square of a quantity R).\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] `Wikipedia entry on the Coefficient of determination\n",
        "                <http://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> from sklearn.metrics import r2_score\n",
        "        >>> y_true = [3, -0.5, 2, 7]\n",
        "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
        "        >>> r2_score(y_true, y_pred)  # doctest: +ELLIPSIS\n",
        "        0.948...\n",
        "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
        "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
        "        >>> r2_score(y_true, y_pred, multioutput='variance_weighted')  # doctest: +ELLIPSIS\n",
        "        0.938...\n",
        "\n",
        "DATA\n",
        "    __ALL__ = ['mean_absolute_error', 'mean_squared_error', 'median_absolu...\n",
        "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}